{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **数据挖掘——Home Credit Default Risk**\n",
    "\n",
    "Authors：李林（3120220938）、杨洋（3220211141）、敬甲男（3220221052）、李翰杰（3120220936）\n",
    "\n",
    "github地址：https://github.com/leealim/kaggle-Home-Credit-Default-Risk\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "df = pd.read_csv('../data/app_tr.csv')\n",
    "df_te = pd.read_csv('../data/app_te.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        if len(list(df[col].unique())) <= 2:\n",
    "            le.fit(df[col])\n",
    "            df[col] = le.transform(df[col])\n",
    "            df_te[col] = le.transform(df_te[col])\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "df_te = pd.get_dummies(df_te)\n",
    "\n",
    "train_labels = df['TARGET']\n",
    "df, df_te = df.align(df_te, join = 'inner', axis = 1)\n",
    "df['TARGET'] = train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "df_te= np.array(df_te)\n",
    "x_test = df_te[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_root, data_label):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data = data_root\n",
    "        self.label = data_label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        labels = self.label[index]\n",
    "        return data, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def get_dataset(x,y):\n",
    "        dataset = MyDataset(x, y)\n",
    "        datas = DataLoader(dataset, batch_size=256, shuffle=False, sampler=None, \\\n",
    "                        batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, \\\n",
    "                        drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)\n",
    "        return datas\n",
    "\n",
    "dataset = MyDataset.get_dataset(x_train,y_train)\n",
    "train_size = int(len(dataset)*0.7)\n",
    "val_size = len(dataset)-train_size\n",
    "train, val = data.random_split(dataset, [train_size, val_size])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(237, 128), nn.ReLU(),\n",
    "                                     nn.Linear(128, 64), nn.ReLU(),\n",
    "                                     nn.Linear(64,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        y_pre = self.encoder(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        y_pre = F.log_softmax(self.encoder(x), dim=1)\n",
    "        loss = F.nll_loss(y_pre, y)\n",
    "        self.log(\"train_loss\", loss,on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "autoencoder = MLP()\n",
    "trainer = L.Trainer(max_epochs=20)\n",
    "trainer.fit(autoencoder, dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder.eval().cuda(device=0)\n",
    "x=torch.from_numpy(x_test[0]).float().unsqueeze(0).cuda(0)\n",
    "res=model(x).data.cpu().numpy()\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=res[0]\n",
    "target=[]\n",
    "for i in res:\n",
    "    if i[0]<0 | i[1]<0 :\n",
    "        \n",
    "    i=i/i.sum()\n",
    "    target.append(i)\n",
    "type(target)\n",
    "target=np.array(target)\n",
    "type(target)\n",
    "#生成答案\n",
    "test = pd.read_csv('../data/app_te.csv')\n",
    "index=test['SK_ID_CURR']\n",
    "ans=pd.DataFrame(target[:,1],columns = ['TARGET'])\n",
    "result=pd.concat([index,ans],axis=1)\n",
    "result.to_csv('../result/'+version+'/submission_n.csv',encoding = 'utf-8',index = 0)\n",
    "result.shape\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
